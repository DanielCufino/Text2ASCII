{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (4.26.0)\n",
      "Requirement already satisfied: requests in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: filelock in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n",
      "1.13.0.dev20220619\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASCIIDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        '''\n",
    "            Path: string of the path to the raw data to convert to a dataset.\n",
    "\n",
    "                The path should a contain a file formatted as an array of JSON objects where the object is of the form:\n",
    "                    {\n",
    "                        \"prompt\": \"...\",\n",
    "                        \"text\": \"...\"\n",
    "                    }\n",
    "\n",
    "            Tokenizer:\n",
    "\n",
    "                The tokenizer used on the prompts and responses of the dataset. Should be GPT2 pre-trained tokenizer.\n",
    "        '''\n",
    "        self.data = None\n",
    "        self.X = []\n",
    "        self.tokenizer = tokenizer\n",
    "        # ID OF THE TOKEN USED TO INDICATE WHEN A RESPONSE BEGINS\n",
    "        self.res_id = 50260 # = self.decode_str(\"<RES>:\")\n",
    "\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "            for entry in self.data[:100]:\n",
    "                prompt = entry['prompt'] \n",
    "                text = entry['text']\n",
    "                self.X.append(f'<BOS> {prompt}\\n\\n<RES>:\\n\\n{text}\\n\\n<EOS>')\n",
    "            # print(test[0])\n",
    "\n",
    "        # for entry in self.data:\n",
    "        #     prompt = entry['prompt']\n",
    "        #     text = entry['text']\n",
    "        #     self.X.append(f'<BOS> {prompt} <bot>: {text} <EOS>')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        print(\"Tokenizing Text...\")\n",
    "        self.X_encoded = self.tokenizer(self.X, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # print(self.X_encoded.size())\n",
    "        # self.X_encoded = self.X_encoded.to(device)\n",
    "        print(\"Done Tokenizing.\")\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        print(self.input_ids.size())\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "        '''\n",
    "            https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer\n",
    "            \n",
    "            1 for tokens that are not masked,\n",
    "            0 for tokens that are masked.\n",
    "        '''\n",
    "        # Mask the ground truth responses so the model can learn.\n",
    "        for i in range(len(self.X)):\n",
    "            res_i = (self.input_ids[i] == self.res_id).nonzero(as_tuple=True)[0]\n",
    "            self.attention_mask[i][res_i + 1:] = 0\n",
    "            # print(res_i)\n",
    "            # print(self.attention_mask[i].sum())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])\n",
    "        # return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx]}\n",
    "        # return (self.X[idx])\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "    \n",
    "    def decode_token(self, token_id):\n",
    "        return self.tokenizer.decoder.get(token_id)\n",
    "    \n",
    "    def decode_str(self, word):\n",
    "        return self.tokenizer.get_vocab()[word]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate our Tokenizer, model, dataset, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Text...\n",
      "Done Tokenizing.\n",
      "torch.Size([100, 1024])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                                \"bos_token\": \"<BOS>\",\n",
    "                                \"eos_token\": \"<EOS>\"})\n",
    "tokenizer.add_tokens([\"<RES>:\"])\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# optim = Adam(model.parameters())\n",
    "\n",
    "ASCII_DATA = ASCIIDataset(\"./raw_data.json\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50260"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ASCII_DATA[0][0].size())\n",
    "\n",
    "# for _ in (ASCII_DATA[0][1]):\n",
    "#     print(_)\n",
    "ASCII_DATA.decode_str(\"<RES>:\")\n",
    "# ASCII_DATA[0]['input_ids'].size()\n",
    "# for _ in range(5):\n",
    "#     print(ASCII_DATA[_][0][:10])\n",
    "#     print(ASCII_DATA[_][1][:20])\n",
    "#     print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ASCII_DATA, batch_size=16) # can be changed to the number of available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    " \n",
    "    e = 1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for _ in tqdm.tqdm(range(e)):\n",
    "\n",
    "        for b_ids, b_masks in dataloader:\n",
    "\n",
    "            # batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n",
    "            # b_ids = batch['input_ids']\n",
    "            # b_masks = batch['attention_mask']\n",
    "            # b_ids = torch.tensor(b_ids.to(device)).unsqueeze(0)\n",
    "            # b_ids = batch['input_ids']\n",
    "            # b_masks = batch['attention_mask']\n",
    "            # b_ids = b_ids.to(device)\n",
    "            # b_labels = b_ids.to(device)\n",
    "            # b_masks = torch.tensor(b_masks).to(device)\n",
    "            b_ids = b_ids.to(device)\n",
    "            b_masks = b_masks.to(device)\n",
    "            b_labels =\n",
    "\n",
    "            print(b_ids.size(), b_masks.size(), b_labels.size())\n",
    "\n",
    "            loss = model(input_ids=b_ids, \n",
    "                         labels=b_labels,\n",
    "                         attention_mask=b_masks).loss()\n",
    "\n",
    "\n",
    "            # print(batch['input_ids'].size())\n",
    "            # print(batch['input_ids'])\n",
    "#           p\n",
    "\n",
    "            # test = torch.ones(16, 1024).type(torch.long).to(device)\n",
    "            # test = torch.ones(1024).type(torch.long).to(device)\n",
    "            # print(test.unsqueeze(0))\n",
    "            # print(test.unsqueeze(0).size())\n",
    "\n",
    "            # loss = model(input_ids=test.unsqueeze(0))\n",
    "            # loss = model(input_ids=batch['input_ids'])\n",
    "\n",
    "\n",
    "            # print(test.size())\n",
    "\n",
    "            # loss = torch.zeros(100, 1023)\n",
    "\n",
    "            # print(loss)\n",
    "\n",
    "            break\n",
    "\n",
    "        # for (b_ids, b_masks) in dataloader:\n",
    "        #     # b_ids = torch.tensor(b_ids.to(device)).unsqueeze(0)\n",
    "        #     b_ids = torch.tensor(b_ids).unsqueeze(0).to(device)\n",
    "        #     b_labels = b_ids.to(device)\n",
    "        #     b_masks = torch.tensor(b_masks).unsqueeze(0).to(device)\n",
    "\n",
    "        #     print(b_ids.size(), b_masks.size(), b_labels.size())\n",
    "\n",
    "        #     loss = model(**b_ids, \n",
    "        #                 #  labels=b_labels,\n",
    "        #                  attention_mask=b_masks).loss()\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "            \n",
    "        torch.save(model.state_dict(), f\"model_state_{_}.pt\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(prompt, text):\n",
    "    inp = f'<BOS> {prompt}\\n<RES>:\\n{text}\\n<EOS>'\n",
    "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "    x = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    output = model.generate(x, attention_mask=a)\n",
    "    output = tokenizer.decode(output[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 5000\n",
    "MAX_SEQ_LEN = 1024\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024]) torch.Size([16, 1024]) torch.Size([16, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensors must be 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielcufino/Desktop/S23/COMP 646/project/Text2ASCII.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, optim, dataloader)\n",
      "\u001b[1;32m/Users/danielcufino/Desktop/S23/COMP 646/project/Text2ASCII.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     b_labels \u001b[39m=\u001b[39m b_ids\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(b_ids\u001b[39m.\u001b[39msize(), b_masks\u001b[39m.\u001b[39msize(), b_labels\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49mb_ids, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                  labels\u001b[39m=\u001b[39;49mb_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                  attention_mask\u001b[39m=\u001b[39;49mb_masks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_state_\u001b[39m\u001b[39m{\u001b[39;00m_\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1046\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1047\u001b[0m     input_ids,\n\u001b[1;32m   1048\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1049\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1050\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1051\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1052\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1053\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1054\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1055\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1056\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1057\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1058\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1059\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1060\u001b[0m )\n\u001b[1;32m   1061\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1063\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:889\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    879\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    880\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    881\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    890\u001b[0m         hidden_states,\n\u001b[1;32m    891\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    892\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    893\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    894\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    895\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    896\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    897\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    900\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    388\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 389\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    390\u001b[0m     hidden_states,\n\u001b[1;32m    391\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    392\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    393\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    394\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    395\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    396\u001b[0m )\n\u001b[1;32m    397\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    398\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:311\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    309\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_attn(hidden_states)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    313\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    314\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/transformers/pytorch_utils.py:112\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    111\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m    113\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensors must be 2-D"
     ]
    }
   ],
   "source": [
    "train(model, optim, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<BOS> Apple emoji of grinning face <RES>: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@@@@@.;+++****++++;@@@@@@@@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@@@+++************+++@@@@@@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@+++***??%%%%%%??***+++@@@@@@@@@@@@@\n",
    "@@@@@@@@@@@@+++**?%SS#########S%?**+++@@@@@@@@@@@\n",
    "@@@@@@@@@@@++**?%S###############S?**++@@@@@@@@@@\n",
    "@@@@@@@@@+++**%S##################S%?*+++@@@@@@@@\n",
    "@@@@@@@@:++*?S##S###############SS##S?*++:@@@@@@@\n",
    "@@@@@@@*++*?SSSSSSSSS#########SSSSSSSS?*++*@@@@@@\n",
    "@@@@@@@++*?SSSSSSSSSSSSSSSSSSSSSSSSSSSS?*++@@@@@@\n",
    "@@@@@@++**%SSSSSSSSSSSSSSSSSSSSSSSSSSSSS?*++@@@@@\n",
    "@@@@@++**%%%SSSSSSSSSSSSSSSSSSSSSSSSSS%%%**++@@@@\n",
    "@@@@@++*?%%%%SSSSSSSSSSSSSSSSSSSSSSS%%%%%%*++@@@@\n",
    "@@@@++*?%%%%%%%%%%%%SSSSSSSSS%%%%%%%%%%%%%?*++@@@\n",
    "@@@@++*?%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%?*++@@@\n",
    "@@@;++*??%%%%%%%%??%%%%%%%%%%%?*?%%%%%%%%???*+;@@\n",
    "@@@++*?????%%%%%+,,;%%%%%%%%%?:,:*%%%%%?????*++@@\n",
    "@@:++*???????%%*,,,,+%%%%%%%%;,:,:?%%???????*++;@\n",
    "@@+++*????????%+,:::;%%%%%%%?::::,*?????????*++;@\n",
    "@@;++*?????????+::;:;?????%%?::;::*?????????*++;@\n",
    "@@+++**????????*::::+????????;::::?????????**+++@\n",
    "@@+++**?????????+::;?????????*:::*?????????**++;@\n",
    "@@;++***?????????**????????????*??????????***++;@\n",
    "@@;++***??????????????????????????????????***++;@\n",
    "@@;++***??????????????????????????????????***++;@\n",
    "@@;++***??????????????????????????????????***++;@\n",
    "@@;++***?**???????????????????????????*******++;@\n",
    "@@;;+***?+,:+**????????????????????*+;:,*?***++;@\n",
    "@@;;++***;:;,,::;++****????****++;:,,:;,+***++;;@\n",
    "@@;;++***;;S%?*;:,,,,,,,,,,,,,,,,:;*?%?:****++;;@\n",
    "@@?;+++**+:%SSSSS%%??*++++++*??%%SSSSS*:***+++;+@\n",
    "@@@;;++***:*SSSS##################SSS%;;***++;;@@\n",
    "@@@;;+++**;:*?%SS################SS%?+:***+++;;@@\n",
    "@@@@;;++***::::+*?%SS######SSS%?*+;::,;**+++;;@@@\n",
    "@@@@;;+++**+::::::::;;;;;;;;;:::::::::***+++;;@@@\n",
    "@@@@:;;+++**;::;::::::::::::::::::;::+**+++;;@@@@\n",
    "@@@@@;;;+++**;::::::::::::::::::::::+**+++;;;@@@@\n",
    "@@@@@@;;;+++**;::::::::::::::::::::+**+++;;;@@@@@\n",
    "@@@@@@@;;;+++**+:::::::::::::::::;***+++;;;@@@@@@\n",
    "@@@@@@@+;;;++++**;:::::::::::::;+**++++;;;+@@@@@@\n",
    "@@@@@@@@*;;+++++***+;::::::::;+***++++;;;*@@@@@@@\n",
    "@@@@@@@@@;;;++++++****++++++****++++++;;+@@@@@@@@\n",
    "@@@@@@@@@@@;;;++++++++******++++++++;;;@@@@@@@@@@\n",
    "@@@@@@@@@@@@;;;;++++++++++++++++++;;;+@@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@;;;;;++++++++++++;;;;;@@@@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@@@;;;;;;;;;;;;;;;;;;@@@*?@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@;+@@+;;;;;;;;;;;;+@@@@@@@@@@@@@@@@@\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<BOS> ASCII art of an emoji of grinning face <RES>: %?????******?????%                    \n",
    "                S???*****++;;;;;;++*****???S               \n",
    "             S???**++;::............::;;+**???%            \n",
    "          ???**+;::.....................:;+**???          \n",
    "        ???**+::::::::..............::::::::;**???        \n",
    "       %??**+;:::::::::::::::::::::::::::::::::;**??S      \n",
    "     ???*+;::::::::::::::::::::::::::::::::::;:;+*???     \n",
    "    ??**+;;;;;;;;;;;::::::::::::::::::;;;;;;;;;;;+*???    \n",
    "    %??*+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;+*??%   \n",
    "   S??**+++++;;;;;;*%SS%+;;;;;;;;;;*%SS%+;;;;;;++++++*???  \n",
    "  ???*++++++++++;*#SSS#%+;;;;;;;;*##SS#S++++++++++++*??%  \n",
    "  *%??**++++++++++*#SSSS%++++++++;*SSSSSS+++++++++++**??%? \n",
    " ????***++++++++++*%%%%+++++++++++*%%%%*++++++++++***??%% \n",
    "  %%??****+++++++++++++++++++++++++++++++++++++++++***??%% \n",
    "  %%??***+++++++++++++++++++++++++++++++++++++++++****??%% \n",
    " ?%???***+*?**++++++++++++++++++++++++++++++**?*+***???%% \n",
    "  *%???****%#%%S%%%???****************???%%%S%S#?****???%? \n",
    "   %%???***%S;;;++**?%%%%%%SSS%SS%%%%%%??*+++;*S****???%%  \n",
    "   *%%???***S%;:::........................::;+S?***???%%?  \n",
    "   ?%%???***SS%??*++;::..........:::;++**?%%S%***???%%?   \n",
    "    ?%%????**%SSSSSSSSS%%%%%%%%%%%SSSSSSSSSS?***???%%%    \n",
    "      %%%????**?%SSSSSSSSSSSSSSSSSSSSSSSSSS%***????%%?     \n",
    "      ?%%%????**?%SSSSSSSSSSSSSSSSSSSSS%?***????%%%*      \n",
    "         %%%%?????**??%%SSSSSSSSSSSS%%?***?????%%%%        \n",
    "           %%%%??????***???????????****??????%%%%          \n",
    "            ?%%%%%???????????*??????????%%%%%%            \n",
    "                %%%%%%%%%??????????%%%%%%%%%               \n",
    "                     %%%%%%%%%%%%%%%%%%S <EOS>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
