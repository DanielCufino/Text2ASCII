{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (4.26.0)\n",
      "Requirement already satisfied: requests in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: filelock in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/danielcufino/miniforge3/envs/torch/lib/python3.8/site-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielcufino/Desktop/S23/COMP 646/project/Text2ASCII.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(transformers\u001b[39m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASCIIDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        '''\n",
    "            Path: string of the path to the raw data to convert to a dataset.\n",
    "\n",
    "                The path should a contain a file formatted as an array of JSON objects where the object is of the form:\n",
    "                    {\n",
    "                        \"prompt\": \"...\",\n",
    "                        \"text\": \"...\"\n",
    "                    }\n",
    "\n",
    "            Tokenizer:\n",
    "\n",
    "                The tokenizer used on the prompts and responses of the dataset. Should be GPT2 pre-trained tokenizer.\n",
    "        '''\n",
    "        self.data = None\n",
    "        self.X = []\n",
    "        self.tokenizer = tokenizer\n",
    "        # ID OF THE TOKEN USED TO INDICATE WHEN A RESPONSE BEGINS\n",
    "        self.res_id = 50260 # = self.decode_str(\"<RES>:\")\n",
    "\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "            for entry in self.data[:100]:\n",
    "                prompt = entry['prompt']\n",
    "                text = entry['text']\n",
    "                self.X.append(f'<BOS> {prompt}\\n<RES>:\\n{text}\\n<EOS>')\n",
    "            # print(test[0])\n",
    "\n",
    "        # for entry in self.data:\n",
    "        #     prompt = entry['prompt']\n",
    "        #     text = entry['text']\n",
    "        #     self.X.append(f'<BOS> {prompt} <bot>: {text} <EOS>')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        print(\"Tokenizing Text...\")\n",
    "        self.X_encoded = self.tokenizer(self.X, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # print(self.X_encoded.size())\n",
    "        # self.X_encoded = self.X_encoded.to(device)\n",
    "        print(\"Done Tokenizing.\")\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        print(self.input_ids.size())\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "        '''\n",
    "            https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer\n",
    "            \n",
    "            1 for tokens that are not masked,\n",
    "            0 for tokens that are masked.\n",
    "        '''\n",
    "        # Mask the ground truth responses so the model can learn.\n",
    "        for i in range(len(self.X)):\n",
    "            res_i = (self.input_ids[i] == self.res_id).nonzero(as_tuple=True)[0]\n",
    "            self.attention_mask[i][res_i + 1:] = 0\n",
    "            # print(res_i)\n",
    "            # print(self.attention_mask[i].sum())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])\n",
    "        # return (self.X[idx])\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "    \n",
    "    def decode_token(self, token_id):\n",
    "        return self.tokenizer.decoder.get(token_id)\n",
    "    \n",
    "    def decode_str(self, word):\n",
    "        return self.tokenizer.get_vocab()[word]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate our Tokenizer, model, dataset, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Text...\n",
      "Done Tokenizing.\n",
      "torch.Size([100, 1024])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                                \"bos_token\": \"<BOS>\",\n",
    "                                \"eos_token\": \"<EOS>\"})\n",
    "tokenizer.add_tokens([\"<RES>:\"])\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# optim = Adam(model.parameters())\n",
    "\n",
    "ASCII_DATA = ASCIIDataset(\"./raw_data.json\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "tensor([50258, 16108, 44805,   286, 45702,  1986, 50260, 37991, 37991, 37991])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([50258, 11708, 44805,   286, 45702,  1986, 50260, 37991, 37991, 37991])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([50258, 12025, 44805,   286, 45702,  1986, 50260, 37991, 37991, 37991])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([50258, 11209, 44805,   286, 45702,  1986, 50260, 37991, 37991, 37991])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([50258, 14254, 44805,   286, 45702,  1986, 50260, 37991, 37991,    31])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ASCII_DATA[0][0].size())\n",
    "\n",
    "# for _ in (ASCII_DATA[0][1]):\n",
    "#     print(_)\n",
    "ASCII_DATA.decode_str(\"<RES>:\")\n",
    "len(ASCII_DATA[0])\n",
    "for _ in range(5):\n",
    "    print(ASCII_DATA[_][0][:10])\n",
    "    print(ASCII_DATA[_][1][:20])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ASCII_DATA, batch_size=16) # can be changed to the number of available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    " \n",
    "    e = 1\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for _ in tqdm.tqdm(range(e)):\n",
    "\n",
    "        for (b_ids, b_masks) in dataloader:\n",
    "            # b_ids = torch.tensor(b_ids.to(device)).unsqueeze(0)\n",
    "            b_ids = torch.tensor(b_ids).unsqueeze(0).to(device)\n",
    "            b_labels = b_ids.to(device)\n",
    "            b_masks = torch.tensor(b_masks).unsqueeze(0).to(device)\n",
    "\n",
    "            print(b_ids.size(), b_masks.size(), b_labels.size())\n",
    "\n",
    "            loss = model(**b_ids, \n",
    "                        #  labels=b_labels,\n",
    "                         attention_mask=b_masks).loss()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        torch.save(model.state_dict(), f\"model_state_{_}.pt\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(prompt, text):\n",
    "    inp = f'<BOS> {prompt}\\n<RES>:\\n{text}\\n<EOS>'\n",
    "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "    x = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    output = model.generate(x, attention_mask=a)\n",
    "    output = tokenizer.decode(output[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 5000\n",
    "MAX_SEQ_LEN = 1024\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/18/4xm448kx703ccdk82hlxhg_80000gp/T/ipykernel_58412/1992965073.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b_ids = torch.tensor(b_ids).unsqueeze(0).to(device)\n",
      "/var/folders/18/4xm448kx703ccdk82hlxhg_80000gp/T/ipykernel_58412/1992965073.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b_masks = torch.tensor(b_masks).unsqueeze(0).to(device)\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 1024]) torch.Size([1, 16, 1024]) torch.Size([1, 16, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GPT2LMHeadModel object argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielcufino/Desktop/S23/COMP 646/project/Text2ASCII.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, optim, dataloader)\n",
      "\u001b[1;32m/Users/danielcufino/Desktop/S23/COMP 646/project/Text2ASCII.ipynb Cell 16\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m b_masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(b_masks)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(b_ids\u001b[39m.\u001b[39msize(), b_masks\u001b[39m.\u001b[39msize(), b_labels\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mb_ids, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m              labels\u001b[39m=\u001b[39mb_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m              attention_mask\u001b[39m=\u001b[39mb_masks)\u001b[39m.\u001b[39mloss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielcufino/Desktop/S23/COMP%20646/project/Text2ASCII.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mTypeError\u001b[0m: GPT2LMHeadModel object argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "train(model, optim, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
